{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Pre-Activation Wide-ResNet Network for Spectrum Mining and Modulation Detection.\n",
    "\n",
    "References:\n",
    "    Basic ResNet: https://arxiv.org/pdf/1512.03385.pdf\n",
    "    Pre-Act. ResNet: https://arxiv.org/pdf/1603.05027.pdf\n",
    "    Wide-ResNet: https://arxiv.org/pdf/1605.07146v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create a decorator that wraps around our inference, loss, and optimizer subgraphs.\n",
    "This makes sure we only call the graph creation code once. Small, but handy. Can extend this to work with `tf.variable_scope`. This combined with `partial` can really cut down on boilerplate code and make the network much more readable/understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def lazy_init(function):\n",
    "    '''\n",
    "    Lazy initialization for inference, loss, and optimzier in a graph.\n",
    "    '''\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a named tuple for our hyperparameters. This is cleaner and less error-prone than passing in a dictionary. Combined with `FLAGS`, it is the ideal way of handling the multitude of parameters in a Deep Learning project.\n",
    "\n",
    "We also define some constants for Batch Normalization that have shown to be reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# tuple for model hyperparameters\n",
    "HyperParams = namedtuple('HyperParams',\n",
    "                     'batch_size, num_cls, num_chans_in, height, width, filter_dims,'\n",
    "                     'lr, kernel_dims, strides, n_hidden, relu_alpha')\n",
    "# sample hyperparameters\n",
    "h = HyperParams(128, 10, 1, 20, 512, [16, 16, 16, 32, 64], 1e-5, [3, 3],\n",
    "                [1, 1, 1, 2, 2], 128, 0.01)\n",
    "\n",
    "# parameters for batch_norm\n",
    "_BATCH_NORM_DECAY = 0.997\n",
    "_BATCH_NORM_EPSILON = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to build on Pre-Activation ResNet in tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def ResNet(object):\n",
    "    def __init__(self, hps=h):\n",
    "        '''\n",
    "        Builds a 5-layer pre-activation ResNet.\n",
    "        Using Xavier initializer for conv and dense weights.\n",
    "        Leaky-relu as activation function.\n",
    "        '''\n",
    "        # parameters and inputs\n",
    "        self.batch_size = 128 # hps.batch_size\n",
    "        self.num_cls = 10 # hps.num_cls\n",
    "        num_chans = 1 # hps.num_chans_in\n",
    "        height = 20 # hps.height\n",
    "        width = 512 # hps.width\n",
    "\n",
    "        # placeholder for inputs, labels, and is_training flag\n",
    "        self.inputs = tf.placeholder(tf.float32,\n",
    "            shape=(self.batch_size, num_chans, height, width),\n",
    "            name='x_placeholder')\n",
    "        self.labels = tf.placeholder(tf.float32,\n",
    "            shape=(self.batch_size, self.num_cls), name='labels')\n",
    "        self.is_training = tf.placeholder_with_default(True, [],\n",
    "            name='is_training')\n",
    "\n",
    "        # network parameters\n",
    "        self.n_filts = [16, 16, 16, 32, 64] # hps.filter_dims\n",
    "        # self.n_filts = [16, 16, 80, 160, 320] # params for wide-resnet(5,4)\n",
    "        self.kernel_dim = (3, 3) # hps.kernel_dims\n",
    "        self.strides = [1, 1, 1, 2, 2] # hps.strides\n",
    "        self.n_hidden = 128 # hps.n_hidden\n",
    "        self.relu_alpha = 0.01 # hps.relu_alpha\n",
    "        # learning rate\n",
    "        self.lr = 1e-5 # hps.lr\n",
    "        # initializers for weight layers\n",
    "        self.conv_init = tf.contrib.layers.xavier_initializer_conv2d\n",
    "        self.dense_init = tf.contrib.layers.xavier_initializer\n",
    "\n",
    "        # forward pass through resnet\n",
    "        self.inference = self._get_inference()\n",
    "        # get loss from inference and labels\n",
    "        self.loss = self._get_loss()\n",
    "        # get optimizer step to minimize loss\n",
    "        self.optim = self._get_optimizer()\n",
    "\n",
    "    @lazy_init\n",
    "    def _get_inference(self):\n",
    "        x = self._get_first_layer()\n",
    "        x = self._build_model(x)\n",
    "        return x\n",
    "\n",
    "    def _get_first_layer(self):\n",
    "        '''\n",
    "        Return the first convolutional layer common to all ResNet models.\n",
    "\n",
    "        Note: Slightly different from layer + pooling in original resnet, but\n",
    "            this performs better.\n",
    "        '''\n",
    "        with tf.variable_scope('first_layer'):\n",
    "            x = tf.layers.conv2d(self.inputs, filters=self.n_filts[0],\n",
    "                kernel_size=self.kernel_dim, strides=self.strides[0],\n",
    "                kernel_initializer=tf.conv_init, padding='same')\n",
    "            return x\n",
    "\n",
    "    def _build_model(self, x):\n",
    "        '''Builds the residual blocks.'''\n",
    "        # scales better for deeper resnet blocks\n",
    "        # for i in range(1, len(self.n_filts)):\n",
    "        #     with tf.variable_scope('resnet_block_{:d}'.format(i)):\n",
    "        #         x = self._resnet_block(x, self.n_filts[i], self.strides[i],\n",
    "        #             self.training_pl)\n",
    "\n",
    "        with tf.variable_scope('first_block'):\n",
    "            x = self._resnet_block(x, self.n_filts[1], self.strides[1],\n",
    "                    self.training_pl)\n",
    "\n",
    "        with tf.variable_scope('second_block'):\n",
    "            x = self._resnet_block(x, self.n_filts[2], self.strides[2],\n",
    "                    self.training_pl)\n",
    "\n",
    "        with tf.variable_scope('third_block'):\n",
    "            x = self._resnet_block(x, self.n_filts[3], self.strides[3],\n",
    "                    self.training_pl)\n",
    "\n",
    "        with tf.variable_scope('fourth_block'):\n",
    "            x = self._resnet_block(x, self.n_filts[4], self.strides[4],\n",
    "                    self.training_pl)\n",
    "\n",
    "        with tf.variable_scope('GAP_block'):\n",
    "            # NOTE: assumes channels_first\n",
    "            x = tf.reduce_mean(x, [2, 3])\n",
    "\n",
    "        with tf.variable_scope('dense'):\n",
    "            x = tf.reshape(x, [self.batch_size, -1])\n",
    "            x = tf.layers.dense(x, self.n_hidden, activation=self._leaky_relu,\n",
    "                    kernel_initializer=self.dense_init)\n",
    "\n",
    "        with tf.variable_scope('logits'):\n",
    "            x = tf.layers.dense(x, self.num_cls,\n",
    "                    kernel_initializer=self.dense_init)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @lazy_init\n",
    "    def _get_loss(self):\n",
    "        with tf.variable_scope('loss'):\n",
    "            loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=self.labels,\n",
    "                logits=self.inference\n",
    "            )\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        return loss\n",
    "\n",
    "    @lazy_init\n",
    "    def _get_optimizer(self):\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            optim = tf.train.MomentumOptimizer(learning_rate=self.lr,\n",
    "                momentum=.9, use_nesterov=True)\n",
    "            train_op = optim.minimize(self.loss)\n",
    "        return train_op\n",
    "\n",
    "\n",
    "    def _resnet_block(self, inputs, in_filts, strides, is_training):\n",
    "        '''\n",
    "        ResNet block with two convolutions.\n",
    "        Projects original inputs through 1x1 conv if strides downsample.\n",
    "        '''\n",
    "        # store original x for skip-connection\n",
    "        orig_in = inputs\n",
    "\n",
    "        # if we are down-sampling, need to double amount of filters\n",
    "        if strides == 2:\n",
    "            in_filts *= 2\n",
    "\n",
    "        # first pre-activation block\n",
    "        inputs = self._batch_norm_relu(inputs, is_training)\n",
    "        inputs = tf.layers.conv2d(inputs, in_filts, self.kernel_dim, strides,\n",
    "            padding='same', data_format='channels_first',\n",
    "            kernel_initializer=self.conv_init)\n",
    "\n",
    "        # second pre-activation block\n",
    "        inputs = self._batch_norm_relu(inputs, is_training)\n",
    "        inputs = tf.layers.conv2d(inputs, in_filts, self.kernel_dim, strides,\n",
    "            padding='same', data_format='channels_first',\n",
    "            kernel_initializer=self.conv_init)\n",
    "\n",
    "        # add back original inputs, with projection if needed\n",
    "        if strides == 2:\n",
    "            # 1x1 conv with downsample strides and filters\n",
    "            orig_in = tf.conv2d(orig_in, in_filts, (1, 1), strides,\n",
    "                padding='same', data_format='channels_first',\n",
    "                kernel_initializer=self.conv_init)\n",
    "        inputs += orig_in\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def _batch_norm_relu(self, inputs, is_training):\n",
    "        '''\n",
    "        Passes inputs through batch normalization and leaky-relu.\n",
    "        '''\n",
    "        inputs = tf.layers.batch_normalization(inputs, axis=3, momentum=_BATCH_NORM_DECAY,\n",
    "            epsilon=_BATCH_NORM_EPSILON, center=True, scale=True, training=is_training,\n",
    "            fused=True)\n",
    "        inputs = self._leaky_relu(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def _leaky_relu(self, inputs, alpha=0.01):\n",
    "        return tf.maximum(self.relu_alpha * inputs, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.layers.conv2d_transpose??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.losses.softmax_cross_entropy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
