{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "The output is a classifier ( binary )\n",
    "\n",
    "Answer the question of \"What factors will cause a change in classifiecation?\"\n",
    "\n",
    "$$ \\rho(X) = \\frac{e^{\\beta_0 + \\beta_1x}}{1 + e^{\\beta_0 + \\beta_1x}} $$\n",
    "\n",
    "this can be reduced to the following equation\n",
    "\n",
    "$$log(\\frac{\\rho(X)}{1-\\rho(X)}) = \\beta_0 + \\beta_1X$$\n",
    "\n",
    "\n",
    "\n",
    "# When to use this, over others\n",
    "\n",
    "Good for image recognition between two objects\n",
    "\n",
    "If you use linear regression, the predicted values will become greater than one and less than zero if you move far enough on the X-axis. Such values are theoretically inadmissible.\n",
    "\n",
    "One of the assumptions of regression is that the variance of Y is constant across values of X (homoscedasticity). This cannot be the case with a binary variable, because the variance is PQ. \n",
    "\n",
    "When 50 percent of the people are 1s, then the variance is .25, its maximum value. As we move to more extreme values, the variance decreases. When P=.10, the variance is .1*.9 = .09, so as P approaches 1 or zero, the variance approaches zero.\n",
    "\n",
    "The significance testing of the b weights rest upon the assumption that errors of prediction (Y-Y') are normally distributed. Because Y only takes the values 0 and 1, this assumption is pretty hard to justify, even approximately. Therefore, the tests of the regression weights are suspect if you use linear regression with a binary DV\n",
    "\n",
    "\n",
    "\n",
    "#### Evaluating the model using the Log Loss Function\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html\n",
    "\n",
    "\n",
    "#### Univariate feature selection using SelectKBest\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest\n",
    "\n",
    "#### Multivariate feature selection using RFE \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE\n",
    "\n",
    "#### Boruta.py?\n",
    "\n",
    "##### randomized logistic regression\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RandomizedLogisticRegression.html\n",
    "\n",
    "\n",
    "\n",
    "#### how to select values\n",
    "\n",
    "Approaches that naively select model terms based on some p-values or some AIC cut-offs (either in a multivariate model via some kind of stepwise or other selection or by looking at lots of univariate models) lead to extremely problematic fits that may fit the particular dataset well, but will otherwise not be useful. Models constructed in such a fashion tend to wrongly identify variables as relevant that are not (while not identifying truly relevant variables - if we assume the used model is some reasonable approximation to nature, in which some variables are relevant and some are not) and have poor predictive properties on new datasets. Nevertheless such approaches are still often used and one can even occasionally get such work published in some well-respected journals, but are quite thoroughly discredited in the statistical community. There are a lot of more appropriate approaches, e.g. bootstrapping naive model building approaches, cross-validation, random forests, model averaging, variable selection priors etc. that should be used instead.\n",
    "\n",
    "\n",
    "##### feature selection is its own article\n",
    "http://scikit-learn.org/stable/modules/feature_selection.html\n",
    "http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/\n",
    "\n",
    "##### relevant paper\n",
    "https://arxiv.org/pdf/1601.07996v4.pdf\n",
    "\n",
    "# Algorithm Assumptions\n",
    "\n",
    "That the outcome must be discrete, otherwise explained as, the dependent variable should be dichotomous in nature (e.g., presence vs. absent);\n",
    "\n",
    "\n",
    "There should be no outliers in the data, which can be assessed by converting the continuous predictors to standardized, or z scores, and remove values below -3.29 or greater than 3.29.\n",
    "\n",
    "\n",
    "There should be no high intercorrelations (multicollinearity) among the predictors.  This can be assessed by a correlation matrix among the predictors. Tabachnick and Fidell (2012) suggest that as long correlation coefficients among independent variables are less than 0.90 the assumption is met.\n",
    "\n",
    "\n",
    "Logistic regression assumes that the dependent variable is a stochastic event.  For example, if we analyze a pesticides kill rate, the outcome event is either killed or alive.  Since even the most resistant bug can only be either of these two states, logistic regression thinks in likelihoods of the bug getting killed.  If the likelihood of killing the bug is > 0.5 it is assumed dead, if it is < 0.5 it is assumed alive.\n",
    "\n",
    "# Performance Metrics\n",
    "\n",
    "ROC curve and calculate Area Under Curve\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "\n",
    "There are many thousands of tests one can apply to inspect a logistic regression model, and much of this depends on whether one's goal is prediction, classification, variable selection, inference, causal modeling, etc. The Hosmer-Lemeshow test, for instance, assesses model calibration and whether predicted values tend to match the predicted frequency when split by risk deciles. Although, the choice of 10 is arbitrary, the test has asymptotic results and can be easily modified. The HL test, as well as AUC, have (in my opinion) very uninteresting results when calculated on the same data that was used to estimate the logistic regression model. It's a wonder programs like SAS and SPSS made the frequent reporting of statistics for wildly different analyses the de facto way of presenting logistic regression results. Tests of predictive accuracy (e.g. HL and AUC) are better employed with independent data sets, or (even better) data collected over different periods in time to assess a model's predictive ability.\n",
    "\n",
    "Another point to make is that prediction and inference are very different things. There is no objective way to evaluate prediction, an AUC of 0.65 is very good for predicting very rare and complex events like 1 year breast cancer risk. Similarly, inference can be accused of being arbitrary because the traditional false positive rate of 0.05 is just commonly thrown around.\n",
    "\n",
    "If I were you, your problem description seemed to be interested in modeling the effects of the manager reported \"obstacles\" in investing, so focus on presenting the model adjusted associations. Present the point estimates and 95% confidence intervals for the model odds ratios and be prepared to discuss their meaning, interpretation, and validity with others. A forest plot is an effective graphical tool. You must show the frequency of these obstacles in the data, as well, and present their mediation by other adjustment variables to demonstrate whether the possibility of confounding was small or large in unadjusted results. I would go further still and explore factors like the Cronbach's alpha for consistency among manager reported obstacles to determine if managers tended to report similar problems, or, whether groups of people tended to identify specific problems.\n",
    "\n",
    "I think you're a bit too focused on the numbers and not the question at hand. 90% of a good statistics presentation takes place before model results are ever presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is about fitting a model and RLR is about finding the variables that go into the model.\n",
    "\n",
    "Vanilla logistic regression is a generalized linear model. For a binary response, we posit that the log odds of the response probability is a linear function of a number of predictors. Coefficients of the predictors are estimated using maximum likelihood and inference about the parameters is then based on large sample properties of the model. For best results, we typically assume that the model is fairly simple and well understood. We know what independent variables impact the response. We want to estimate the parameters of the model.\n",
    "\n",
    "Of course, in practice, we don't always know what variables should be included in the model. This is especially true in machine learning situations where the number of potential explanatory variables is huge and their values are sparse.\n",
    "\n",
    "Over the years, many people have tried to use the techniques of statistical model fitting for the purpose of variable (read \"feature\") selection. In increasing level of reliability:\n",
    "\n",
    "Fit a big model and drop variables with non-significant Wald statistics. Doesn't always produce the best model.\n",
    "Look at all possible models and pick the \"best\". Computationally intensive and not robust.\n",
    "Fit the big model with an L1 penalty term (lasso style). Useless variables get dropped in the fit. Better, but unstable with sparse matrices.\n",
    "Randomize method 3. Take random subsets, fit a penalized model to each and collate the results. Variables that come up frequently are selected. When the response is binary, this is randomized logistic regression. A similar stunt can be pulled with continuous data and the general linear model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
