{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "Linear regression aims to find the relationship between a dependent variably Y and independent variable X\n",
    "\n",
    "X is sometimes referred to as an explanatory variablee.\n",
    "\n",
    "When used to predict, $X$ is called a feature.. and the prediction is called a response $Y$\n",
    "\n",
    "with multiple linear regression you use multiple features ($x_0, x_1, x_2$) to predict a single $Y$\n",
    "\n",
    "the number of $X$'s we use is usually called the dimensionality, which is denoted by the variable $D$\n",
    "\n",
    "this means that we are usually working with a $NxD$ matrix, of samples x dimensions\n",
    "\n",
    "this contrary to linear algebra, where vectors are column vectors\n",
    "\n",
    "here they are row vectors\n",
    "\n",
    "golden rule : matrix multiplication inner dimensions must match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>The Multiple Linear Regression Equation</center>\n",
    "### $$\\hat{y} = \\beta_1x_1 + \\beta_2x_2 + ... \\beta_ix_i + \\beta_0$$\n",
    "\n",
    "What does each term represent?\n",
    "- $\\hat{y}$ is the response or set of responses\n",
    "- $x_i$ is one in a set of features\n",
    "- $\\beta_i$ is the coefficient for $x_i$ or the 'prediction coefficient', a pair for every feature\n",
    "- $\\beta_0$ is the intercept or bias\n",
    "\n",
    "need to make sure this question is correct\n",
    "ss sum squares?\n",
    "$$B_i = \\frac{\\sum((x_i-\\bar{x})(y_i-\\bar{y}))}{\\sum(x_iâ€“\\bar{x}^2)} = \\frac{SS_{xy}}{SS_x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics\n",
    "\n",
    "These statistics are meant to help you evaluate the strength of the relationship between input and output. The model will give you a weighted influence that each feature has on the output variable\n",
    "\n",
    "The statistics are explained below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection (this is becoming an article of its own)\n",
    "\n",
    "\n",
    "NOTE: CV depends on the data being IID? or Time Series? or some other type of data?? wtf?\n",
    "\n",
    "when working with multiple features, we should determine which features explain our data, and which are misleading or unrelated\n",
    "\n",
    "to do this we try to make a few different models, choosing different features\n",
    "\n",
    "\n",
    "## A bad approach\n",
    "\n",
    "first we get our P-Values for each feature to get an idea of which features are most likely to be helpful\n",
    "\n",
    "then we play with our model adding and subtracting features and comparing the R2 value, selecting the model with the highest R2 and lowest P-Val\n",
    "\n",
    "Not the best approach because: R2 isnt the best predictor and is prone to overfitting, \n",
    "\n",
    "R2 will also, ALWAYS increase with more features.. you have to judge whether the increase is big enough to justify the potentially higher bias / p-value\n",
    "\n",
    "and P-Val can be subjective and prone to misinterpretation\n",
    "\n",
    "\n",
    "## better approach ( cross validation )\n",
    "\n",
    "cross validation is used to attempt to 'generalize' a model and avoid 'overfitting'\n",
    "\n",
    "the concept is that if the model can work consistently on multiple 'datasets' then it is probably pretty 'general'\n",
    "\n",
    "multiple 'datasets' are generated by splitting and mixing the whole dataset in different ways, this leads to a lot of types of CV\n",
    "\n",
    "### non-exhaustive CV\n",
    "do not compute every possible split method\n",
    "\n",
    "##### holdout method\n",
    "this is the traditional split of training / test data where the training data is used to model and the test data is used to validate\n",
    "\n",
    "\n",
    "##### repeated random sub-sampling CV\n",
    "also called Monte-Carlo CV randomly splits the data set into training/test.\n",
    "\n",
    "this can happen multiple times\n",
    "\n",
    "the average accuracy of each pair is taken\n",
    "\n",
    "benefit: no static number of folds\n",
    "\n",
    "cons: some data may not be used to validate, some may be used more than once\n",
    "another con: results can vary per execution since the splits are random\n",
    "this is called \"monte carlo variation\"\n",
    "\n",
    "this method tends toward LpO as the number of splits becomes larger\n",
    "\n",
    "a modificaton of this method is known as 'stratified RRSSCV' where the random samples are seleted by the mean response value, Y, being equal in train/test \n",
    "\n",
    "\n",
    "##### k-fold CV\n",
    "It divides your dataset in to n folds and in each iteration it leaves one of the folds out as the test set and trains the model on the rest of the folds (n-1 folds). So, in the end you will get predictions for the entire data.\n",
    "\n",
    "this is the method used by scikit-learn and the most popular CV method (10-Fold CV)\n",
    "\n",
    "if k=n then this method is the same as LOO CV (LpO where p=1)\n",
    "\n",
    "### exhaustive CV\n",
    "compute every possible split method\n",
    "\n",
    "#### leave P out CV \n",
    "\n",
    "LpO takes p number of observations as the validation set and leaves the rest for training\n",
    "\n",
    "this loops repeatedly until the slices of p have covered the entire set of possible sets\n",
    "\n",
    "if n=100 and p=30, this would take 3x10^25 computations (n is the number of observations)\n",
    "\n",
    "C 30 over 100\n",
    "\n",
    "##### leave one out CV\n",
    "\n",
    "this method is a special case of leave p out where p=1.\n",
    "\n",
    "the idea is to slice out a sample on the left out piece\n",
    "\n",
    "it is much faster than LpO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "in binary (classification) the Positive Predictive Value can be used to determine fit\n",
    "\n",
    "in continuout (probability prediction) the RMSE or median absolute deviation can be used\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The null hypothesis and the P-Value\n",
    "\n",
    "what if your prediction is completely wrong? And there is no relation\n",
    "\n",
    "thats what P-Value tries to tell you, P ranges from 0 to 1\n",
    "\n",
    "A high p-value tries to tell you that you are compatible with the null hypothesis, meaning your prediction is probably wrong\n",
    "\n",
    "A low p-value shows that your prediction is not in line with the null hypothesis, meaning your prediction may be correct\n",
    "\n",
    "Example a P-Value of .05, means that if you reject the null hypothesis.. there is a 5% chance you are wrong.\n",
    "\n",
    "Its good practice to multiple your P-Value rate by ~5x. so .05 would be better thought of as 25% wrong\n",
    "\n",
    "As a note, repeatability is always the most important thing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R: Correlation Coefficient\n",
    "\n",
    "The r value is the measure of how closely the two inputs are related.\n",
    "\n",
    "r ranges from -1 to +1 and the closer it is to either side, the closer the variables are related\n",
    "\n",
    "the variables are either posivitely correlated: increasing together (+1), inversley correlated: increasing oppositely (-1) or not corelated: unpredictable (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $R^2$: How well does my model explain the variance\n",
    "\n",
    "\n",
    "Squaring R gives you a % of how well the model explains the variance\n",
    "an R^2 of 1 means that all the variance was explained by the model\n",
    "R^2 of 0 means none of the variance was explained by the model\n",
    "\n",
    "R-squared cannot determine whether the coefficient estimates and predictions are biased, which is why you must assess the residual plots.\n",
    "\n",
    "R-squared does not indicate whether a regression model is adequate. You can have a low R-squared value for a good model, or a high R-squared value for a model that does not fit the data!\n",
    "\n",
    "R^2 is generally not the best stat to use for making assumptions about your model\n",
    "https://stats.stackexchange.com/questions/13314/is-r2-useful-or-dangerous\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# need to mention F-Test and T-Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slope and Intercept\n",
    "\n",
    "the slope and intercept are used to create the function that allows you to make predictions about new data\n",
    "\n",
    "using the 'model' equation you can plug in a new X to find the predicted Y for that X\n",
    "\n",
    "if your statistics show a strong relationship between input and output, then your prediction has a good chance of being meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considering the residual\n",
    "### $$\\hat{y} = \\beta_1x_1 + \\beta_2x_2 + ... \\beta_ix_i + \\beta_0 + \\epsilon$$\n",
    "\n",
    "The residual $\\epsilon$ is calculated as the difference between the dependant variable $y$, and the prediction $\\hat{y}$\n",
    "\n",
    "$$\\epsilon = y - \\hat{y}$$\n",
    "\n",
    "A good way to use this data is to plot it on a graph against the independent variable $x$ and try to detect a pattern.\n",
    "\n",
    "If no pattern exists, your modeel probably does a fairly good job explaining the data or at the very least, there is no obviously missing feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
