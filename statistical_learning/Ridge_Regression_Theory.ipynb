{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Short Description\n",
    "\n",
    "Ridge regression, often called Tikhonov regularization, is a modification of OLS in which the coefficients are scaled based on a penalty term. This penality term is placed on the square of the coefficient in an attempt to reduce overfitting by the original OLS regression. Ridge regression has two main benefits. First, adding a penalty term reduces overfitting. Second, the penalty term guarantees that we can find a solution.\n",
    "\n",
    "\n",
    "The ridge regression has two important advantages over the linear regression. The most important one is that it penalizes the estimates. It doesn't penalize all the feature’s estimate arbitrarily. If estimates (β)β) value are very large, then the SSE term in the above equation will minimize, but the penalty term will increases. If estimates(β)β)values are small, then the penalty term in the above equation will minimize, but, the SSE term will increase due to poor generalization. So, it chooses the feature's estimates (ββ) to penalize in such a way that less influential features (Some features cause very small influence on dependent variable) undergo more penalization. In some domains, the number of independent variables is many, as well as we are not sure which of the independent variables influences dependent variable. In this kind of scenario, ridge regression plays a better role than linear regression.\n",
    "\n",
    "\n",
    " The squared penalty term in ridge regression can be seen as a relaxation of this L1 Regularization (LASSO) and the theoretical L0 Regularization (Computationally infeasable) and is computationally very easy to solve.\n",
    "\n",
    "\n",
    "If we consider the RSS equation for a multivariable equation\n",
    "$$ RSS = \\sum_{i=1}^{n}(y_{i}- \\beta_0 - \\sum_{ j = 1}^{p} x_{ij}\\beta_j)^{2} $$\n",
    "\n",
    "We simply modify  the RSS equation to solve for a new Beta that modifies the square of the original beta using a tuning parameter  $\\lambda$\n",
    "$$ \\beta_R = RSS + \\lambda \\sum_{ p = 1}^{j} \\beta_{j}^{2} $$\n",
    "\n",
    "The additional term $$\\lambda \\sum_{p = 1}^{j} \\beta_{j}^{2}$$ is used to shrink the coefficients, and can be tuned to have varying levels of impact, as \\lambda increases it forces the coefficient toward zero. Choosing lambda is usually left to cross-validation techniques instead of guesses. You will notice when $\\lambda=0$ the equation is just RSS. This scaling on the squared coefficient is called L2 Regularizaton. \n",
    "\n",
    "\n",
    "# When to use this, over others\n",
    "\n",
    "In this way you can see that ridge regression is just a scaling of OLS which means it will never zero out coefficients (cant be used to reduce dimensions) unlike LASSO which adds substracts some constant from the coefficient term. Ridge Regression is a technique used when the data suffers from multicollinearity ( independent variables are highly correlated).\n",
    "\n",
    "###### Define Collinear\n",
    "In statistics, multicollinearity (also collinearity) is a phenomenon in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be linearly predicted from the others with a substantial degree of accuracy. In this situation the coefficient estimates of the multiple regression may change erratically in response to small changes in the model or the data. Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set; it only affects calculations regarding individual predictors. That is, a multiple regression model with correlated predictors can indicate how well the entire bundle of predictors predicts the outcome variable, but it may not give valid results about any individual predictor, or about which predictors are redundant with respect to others.\n",
    "###### End\n",
    "\n",
    "\n",
    "\n",
    "It is fallacy to say \"always\", but it has been shown that LASSO Regression and by extension ElasticNet Regression, generally perform better than Ridge Regression. LASSO is very similar to RR but uses L1 regularization, wheras ElasticNet Regression is a combination of LASSO and Ridge and uses both L1 and L2 Regularization.\n",
    "\n",
    "Ridge Regression then cannot produce a sparce solution ( lesser number of features )\n",
    "\n",
    "An analogy to a bathroom scale:  With ridge regression, we accept a scale that is, on average, 2 pounds light, but never more than 2.5 pounds light or less than 1.5 pounds light instead of a scale that is, on average, correct, but often off by a lot.\n",
    "\n",
    "# Algorithm Assumptions\n",
    "\n",
    " under assumptions of uncorrelated, equal-variance, zero-mean errors, OLS estimate is  unbiased and has minimum variance (BLUE). Gauss-Markov theorem\n",
    "\n",
    "# Basic Mechanics\n",
    "\n",
    "Ridge regression is a method of decreasing the variance of regression parameters by accepting some bias in them.\n",
    "L2 regularization means that weights are penalized for growing too large. \n",
    "\n",
    "# Performance Metrics\n",
    "\n",
    "Use $\\lambda$ to minimize RSS and use the model with the lowest MSE by cross validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
