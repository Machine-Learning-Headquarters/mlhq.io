{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Pre-Activation Wide-ResNet Network for Spectrum Mining and Modulation Detection.\n",
    "\n",
    "References:\n",
    "    Basic ResNet: https://arxiv.org/pdf/1512.03385.pdf\n",
    "    Pre-Act. ResNet: https://arxiv.org/pdf/1603.05027.pdf\n",
    "    Wide-ResNet: https://arxiv.org/pdf/1605.07146v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create a decorator that wraps the subgraphs for our inference, loss, and optimizer.\n",
    "This makes sure we only call the graph creation code once. Small, but handy. Can extend decorator to work with `tf.variable_scope` and arbitrary arguments. This combined with `partial` can really cut down on boilerplate code and make the network much more readable/understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def lazy_init(function):\n",
    "    '''\n",
    "    Lazy initialization for inference, loss, and optimzier in a graph.\n",
    "    '''\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a named tuple for our hyperparameters. This is cleaner and less error-prone than passing in a dictionary. Combined with `FLAGS`, it is the ideal way of handling the multitude of parameters in a Deep Learning project.\n",
    "\n",
    "We also define some constants for Batch Normalization that have proven to be reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# tuple for model hyperparameters\n",
    "ResNetParams = namedtuple('ResNetParams',\n",
    "                     'batch_size, num_cls, num_chans_in, height, width, lr, filter_dims,'\n",
    "                     'kernel_dims, strides, n_hidden, relu_alpha')\n",
    "# sample hyperparameters\n",
    "h = HyperParams(128, 10, 1, 20, 512, 1e-5, [16, 16, 32, 64], [3, 3],\n",
    "                [1, 1, 2, 2], 128, 0.01)\n",
    "\n",
    "# parameters for batch_norm\n",
    "_BATCH_NORM_DECAY = 0.997\n",
    "_BATCH_NORM_EPSILON = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to build on Pre-Activation ResNet in tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ResNet(object):\n",
    "    def __init__(self, inputs, labels, is_training, hps):\n",
    "        '''\n",
    "        Builds a 5-layer pre-activation ResNet.\n",
    "        Using Xavier initializer for conv and dense weights.\n",
    "        Leaky-relu as activation function.\n",
    "        '''\n",
    "        # parameters and inputs\n",
    "        self.batch_size = hps.batch_size\n",
    "        self.num_cls = hps.num_cls\n",
    "        num_chans = hps.num_chans\n",
    "        height = hps.height\n",
    "        width = hps.width\n",
    "\n",
    "        # placeholder for inputs, labels, and is_training flag\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.is_training = is_training\n",
    "\n",
    "        # network parameters\n",
    "        self.n_filts = hps.filter_dims\n",
    "        # self.n_filts = [16, 16, 80, 160, 320] # params for wide-resnet(5,4)\n",
    "        self.kernel_dim = hps.kernel_dims\n",
    "        self.strides = hps.strides\n",
    "        self.n_hidden = hps.n_hidden\n",
    "        self.relu_alpha = hps.relu_alpha\n",
    "        # learning rate\n",
    "        self.lr = hps.lr\n",
    "        # initializers for weight layers\n",
    "        self.conv_init = tf.contrib.layers.xavier_initializer_conv2d()\n",
    "        self.dense_init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        # subgraphs\n",
    "        self.inference\n",
    "        self.loss\n",
    "        self.train_op\n",
    "\n",
    "    @lazy_init\n",
    "    def inference(self):\n",
    "        x = self._build_first_layer()\n",
    "        x = self._build_resnet(x)\n",
    "        return x\n",
    "\n",
    "    def _build_first_layer(self):\n",
    "        '''\n",
    "        Return the first convolutional layer common to all ResNet models.\n",
    "\n",
    "        Note: Slightly different from layer + pooling in original resnet, but\n",
    "            this performs better.\n",
    "        '''\n",
    "        with tf.variable_scope('first_layer'):\n",
    "            x = self._conv_layer(self.inputs, 16, 3, 1)\n",
    "            return x\n",
    "\n",
    "    def _build_resnet(self, x):\n",
    "        '''Builds the residual blocks.'''\n",
    "\n",
    "        with tf.variable_scope('first_block'):\n",
    "            x = self._resnet_block(x, self.n_filts[0], self.strides[0],\n",
    "                    self.is_training)\n",
    "\n",
    "        with tf.variable_scope('second_block'):\n",
    "            x = self._resnet_block(x, self.n_filts[1], self.strides[1],\n",
    "                    self.is_training)\n",
    "\n",
    "        with tf.variable_scope('third_block'):\n",
    "            x = self._resnet_block(x, self.n_filts[2], self.strides[2],\n",
    "                    self.is_training)\n",
    "\n",
    "        with tf.variable_scope('fourth_block'):\n",
    "            x = self._resnet_block(x, self.n_filts[3], self.strides[3],\n",
    "                    self.is_training)\n",
    "\n",
    "        with tf.variable_scope('GAP_block'):\n",
    "            x = tf.reduce_mean(x, [1, 2])\n",
    "\n",
    "        with tf.variable_scope('dense'):\n",
    "            x = tf.reshape(x, [self.batch_size, -1])\n",
    "            x = tf.layers.dense(x, self.n_hidden, activation=self._leaky_relu,\n",
    "                    kernel_initializer=self.dense_init)\n",
    "\n",
    "        with tf.variable_scope('logits'):\n",
    "            x = tf.layers.dense(x, self.num_cls,\n",
    "                kernel_initializer=self.dense_init)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @lazy_init\n",
    "    def loss(self):\n",
    "        with tf.variable_scope('loss'):\n",
    "            x_ent = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=self.labels,\n",
    "                logits=self.inference)\n",
    "            loss = tf.reduce_mean(x_ent)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "        return loss\n",
    "\n",
    "    @lazy_init\n",
    "    def train_op(self):\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            # learning rate schedule\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                learning_rate=self.lr, global_step=global_step,\n",
    "                decay_steps=10000, decay_rate=0.97, staircase=True)\n",
    "            tf.summary.scalar('learn_rate', learning_rate)\n",
    "            # Nesterov momentum optimizer\n",
    "            optim = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                momentum=.9, use_nesterov=True)\n",
    "            # add optimizers for batch_norm\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                train_op = optim.minimize(self.loss, global_step=global_step)\n",
    "        self.summaries = tf.summary.merge_all()\n",
    "        return train_op\n",
    "\n",
    "    def _resnet_block(self, inputs, num_filts, strides, is_training):\n",
    "        '''\n",
    "        ResNet block with two convolutions.\n",
    "        Projects original inputs through 1x1 conv if strides downsample.\n",
    "        '''\n",
    "        # store original x for skip-connection\n",
    "        orig_in = inputs\n",
    "\n",
    "        # if downsampling, then double number of filters and project original x\n",
    "        if strides == 2:\n",
    "            num_filts *= 2\n",
    "            orig_in = self._conv_layer(orig_in, num_filts, 1, 2*strides)\n",
    "\n",
    "        # first pre-activation block\n",
    "        inputs = self._batch_norm_relu(inputs, is_training)\n",
    "        inputs = self._conv_layer(inputs, num_filts, self.kernel_dim, strides)\n",
    "\n",
    "        # second pre-activation block\n",
    "        inputs = self._batch_norm_relu(inputs, is_training)\n",
    "        inputs = self._conv_layer(inputs, num_filts, self.kernel_dim, strides)\n",
    "\n",
    "        # add back original (possibly projected) inputs\n",
    "        inputs += orig_in\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def _batch_norm_relu(self, inputs, is_training):\n",
    "        '''\n",
    "        Passes inputs through batch normalization and leaky-relu.\n",
    "        '''\n",
    "        inputs = tf.layers.batch_normalization(inputs, axis=3,\n",
    "            momentum=_BN_DECAY, epsilon=_BN_EPSILON, center=True, scale=True,\n",
    "            training=is_training, fused=True)\n",
    "        inputs = self._leaky_relu(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def _conv_layer(self, inputs, n_filts, kernel, strides, padding='same',\n",
    "                    data_format='channels_last'):\n",
    "        return tf.layers.conv2d(inputs, n_filts, kernel, strides, padding=padding,\n",
    "            data_format=data_format, kernel_initializer=self.conv_init)\n",
    "\n",
    "    def _leaky_relu(self, inputs):\n",
    "        return tf.maximum(self.relu_alpha * inputs, inputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
