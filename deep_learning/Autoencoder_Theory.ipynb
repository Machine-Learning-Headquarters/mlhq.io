{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we have samples of training data but no matching set of labels. This can happen for many reasons:\n",
    "    1. Labeled data can be very expensive.\n",
    "    2. Finding data that matches our real world use-cases is difficult.\n",
    "    3. Manually labeling and curating data is very time consuming.\n",
    "The above are a few reasons why getting large amounts of unlabeled data is faster and more economical than collecting a matching, labeled dataset. \n",
    "\n",
    "This begs the question, if so much of the current Deep Learning wave was built on labeled data, is there still room for large, unlabeled datasets? This is the realm of Unsupervised Learning, where we do not learn a mapping from X -> Y because it either does not exist or we do not know it. It is the least common form of learning compared to Classification and Regression and the most loosely defined as well. The broad goal in Unsupervised Learning is to discover some hidden qualities or structure in our data.\n",
    "\n",
    "One of the first methods for training large nets was based on Unsupervised Learning. Before many of the more advanced initialization schemes (i.e. Glorot and He), Deep Nets were initialized with something called `Unsupervised Pre-Training`. The `Restricted Boltzman Machine` (RBM) was at the heart of this initialization scheme and is similar to a two-layer neural network. The first input layer is called the \"visible\" layer while the second layer is called the \"hidden\" layer. The main goal of an RBM is learning a good representation of the input in the hidden layer. This is analogous to the idea of finding those hidden qualities or structures in the unlabeled training data. `Deep Belief Nets`, a sort of precursor to the Deep Feedforward Network of today, were initialized in a greedy layer-by-layer fashion with RBMs reconstructing the inputs at each successive layer. \n",
    "\n",
    "Reconstructing the original input based on the activation of the \"hidden\" layer leads to the idea of an `Autoencoder`. While an RBM is usually only two layers deep an Autoencoder will look more like a typical DNN with multiple layers. The goal of an Autoencoder is the same as an RBM: reconstructing the input. Autoencoders usually have a single layer that is much smaller than the others. This `bottleneck` layer forces the network to compress its internal representation into a reduced dimension. The hope is that the intermediate layers, especially the bottleneck layer, distill and capture the most important aspects of the input.\n",
    "\n",
    "If we want to learn good features that can reconstruct the input, then the best measure of an Autoencoder's performance is how well its output matches the original inputs. We can measure this with the reconstruction error defined as `(original - reconstruction)^2`. We optimize our network by minimizing this reconstruction error with end-to-end learning like other standard DNNs instead of layer-by-layer like an RBM. It is also worth mentioning that the bottleneck activations can be used as features for other networks or models in a variety of ways. \n",
    "\n",
    "A popular Autoencoder architecuture is the `Encoder-Decoder`. The Encoder compresses the input down the bottleneck layer. This could be achieved with a series of dense, convolutional, or recurrent layers. The Decoder is usually a mirrored version of the Encoder that restores the bottleneck back to original input dimensions, although it can also be a completely separate network (i.e. for image captioning and language translation tasks). We will see a simple Encoder-Decoder in the `Python` section of this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoising Autoencoders\n",
    "\n",
    "While the ideas of Autoencoders and Encoder-Decoders are very powerful on the surface, there are many pitfalls to consider. Most datasets capture a small subset of the conditions that networks will face in the real world. Our bottleneck layer will have never seen data like it and will handle it poorly. On the other hand, if we build a large network then it could vastly overfit the training set and perform badly on new data even if it comes from the same source and distribution. \n",
    "\n",
    "One of the first ideas for tackling this problem was `Denoising Autoencoders`. Here we distort and corrupt the original input before feeding it into the network. However, when we calculate the cost we still compare the reconstruction to the original, uncorrupted inputs. This ideally forces the network to learn robust features that sift through the noisy input data. The network will better handle unseen cases since it did not rigidly fit the clean training set. Furthermore, it was shown that applying Gaussian noise at the inputs is similar to L2 (i.e. Energy norm, or weight-decay) regularization [ref]. This also prevents the network from overfitting its training set.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Autoencoders\n",
    "\n",
    "Recently a new type of Autoencoders called `Variational Autoencoders` [Kingma ref] are excelling in a variety of tasks. They are related to the well-established field of Bayesian Inference and fall under the rapidly growing umbrella of Bayesian Deep Learning [refs]. \n",
    "\n",
    "The heart of Bayesian inference is predicting or finding the best model parameters that maximize some probability we are interested in. This could be the probability of a sentence given a series of words, or the probability of a correct image label. Bayesian inference comes from the equation describing the relationship between the likelihood, marginal, and prior probabilities.\n",
    "[Pic of Bayes' equations]\n",
    "\n",
    "The quantity on the left, the posterior, is what we really want but is impossible to directly sample from it in most realistic applications. The key takeaway is that we have most of the variables on the right-hand side of the equation. The likelihood is the probability of our data given the model parameters. The prior represents previous knowledge about the problem and what good parameter values might look like. The marginal represents the overall probability of our data. We do not deal with this last quantity directly but it is used when deriving the KL Divergence cost that must be minimized. Next we describe what makes the Variational Autoencoders different from vanilla flavor autoencoders.  \n",
    "\n",
    "Imagine replacing the bottleneck layer in an autoencoder with two additional latent layers. Now, however, the outputs of the layers have new definitions. They become the statistical means and covariances of a multidimensional diagonal Gaussian distribution. Instead of reconstructing the input from the bottlneck layer, we instead sample from the latent layer, just as we would any other probability distribution, and use that \"code\" to reconstruct the input.     \n",
    "\n",
    "In Variational Autoencoders, the `reparametrization trick` combines the means and covariances with noise and makes it possible to learn good model parameters with backpropagation instead of updating them manually. While it is difficult to visualize good parameter values in higher dimensions, it turns out that a standard Gaussian prior is a good starting point. We then compare the means and covariances our network learned against the Gaussian prior using the KL Divergence formula. KL is not strictly a measure of distance but does capture the overlap between two distributions very well. When t\n",
    "\n",
    "This KL divergence is added to the typical cost of the Autoencoder, such as mean reconstruction error or binary cross-entropy. The parameters of a statistical model are then learned via backpropagation. These learned distributions are proving incredibly useful while adding much needed interpretability to DNN models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Convolutional Autoencoders\n",
    "\n",
    "Convolutional autoencoders use, as the name suggests, convolutional layers to learn features that capture the input well. If we use a bottleneck layer with convolutions, how do we go from a smaller input dimension to a larger one? The answer that makes all the math work out is something called fractionally-strided convolutions, or transpose convolutions. To picture them on an image, imagine inserting zeros in between pixel values, in addition to padding. Then, we can take a regular convolution over this \"up-sampled\", sparse pattern and the output of the convolution will match the dimensions we need to restore the bottleneck layer back to the original input dimensions. There is an excellent guide to convolutional arithmetic here: [ref] .\n",
    "\n",
    "This type of autoencoder can be all-convolutional, meaning they have no max, average, or global pooling layers and downsample with strided convolutions instead. This lets the network learn its own upsampling function which can be ideal for tasks such as image reconstruction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
