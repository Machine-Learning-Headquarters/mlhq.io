{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post starts with an easy to understand definition of KL divergence, followed by some high-level interpretations in different applications, and concludes with an intuitive example using coin flips and dice rolls.\n",
    "\n",
    "\n",
    "Definition\n",
    "\n",
    "The KL Divergence tells us how similar or different two probability distributions are. It can be seen as the \"distance\" from one distribution to the other but is not a formal mathematical distance. First we show the formula and then explain it:\n",
    "\n",
    "[KL Div formula pic]\n",
    "[Explain notation and terms in picture]\n",
    "[Show discrete and continuous versions]\n",
    "\n",
    "Information Theory gives us perhaps the clearest definition of KL Divergence based on entropy. Entropy measures the randomness and amount of information in a message. The higher the entropy, then the higher the randomness while the lower the entropy, the more we can predict the message. KL Divergence can be seen as as the difference between two entropies:\n",
    "\n",
    "[Pic showing KL = H(P, Q) - H(P)]\n",
    "\n",
    "This shows us the randomness that is lost if we use the entropy (aka information) from both P and Q as opposed to only P. Said another way, it is how much less random we become when including the cross-information from both distributions. Adding to this definition, there are many interpretations for what KL divergence means. Next we describe some of the different takes on it. \n",
    "\n",
    "\n",
    "Interpretations\n",
    "\n",
    "The most intuitive explanation comes from the field of inference. First, assume that P is the true distribution of our model's parameters given some data. This is the posterior distribution P(Z | X) (where Z represents parameter values, i.e. weight matrices in DNNs) that we cannot sample from. We instead use a model Q(Z | X) that tries to get as close as possible to P(Z | X). In this case KL measures how well our probability Q approximates the real P and is a cost or penalty we can minimize. \n",
    "\n",
    "If Q is a prior belief we used to hold, and P is the updated posterior distribution after observing some new data, then the KL divergence from Q to P represents the information gain in going from the prior to the posterior.\n",
    "\n",
    "From a coding perspective, KL divergence measures the extra number of bits (aka information loss, because we need more bits to represent the same amount of information) when the approximation Q is used to code instead of the real P. \n",
    "\n",
    "\n",
    "Examples\n",
    "\n",
    "Imagine you sign up for an experiment that is testing how well humans can estimate a probability distribution after seeing many examples from the true distribution. To test this, the researchers place you in a room with a screen. You are told that numbers will flash on the screen one at a time and it is your job to estimate the probability distribution of the numbers. In this case the true distribution P depends on how the researches generate the numbers. You are instead estimating this true distribution with your own probability estimate Q.\n",
    "\n",
    "For starters, the researches generate numbers by flipping a fair coin. This means the true distribution P will be a Bernoulli distribution with 1/2 probability for both heads and tails. They flash \"1\" on the screen if it's heads and \"0\" if it's tails. Next the researches flip the coin 100 times and flash 100 numbers on the screen. In the naive approach we could keep a mental estimate of how many 0's and 1's we saw in total, or how many 0's were seen for every 1. We are only tracking two numbers and let's assume that our memory is relatively decent, so we calculate that 60% of the numbers were 0 and 40% were 1. We use this to say the number is 0 with p = 0.60 and 1 with p = 0.40 which becomes our approximation Q.\n",
    "\n",
    "[Example KL calculation here]\n",
    "\n",
    "The KL Divergence between Q and P tells us how far away our guess is from the real Bernoulli distribution. In this sense it is also telling us how well our Q approximates this real P, because the better we approximate it then the \"closer\" we are to it. \n",
    "\n",
    "If we make a better guess, say Q* with with p = 0.55 for 0 and p = 0.45 for 1, then we are closer to the true distribution P and the KL Divergence decreases. If we had guessed perfectly, i.e. p = 0.5 for both 0 and 1, then the KL Divergence would be 0. This result tell us that two probability distributions are essentially identical when the divergence between them in 0.\n",
    "\n",
    "Next, the researchers generate numbers by rolling a fair die. The true distribution P is now uniform between 1 and 6 with an equal probability (p = 1/6) for each number. They flash whichever number the dice landed on across the screen. On your side, it is much harder to mentally track six numbers compared to two. This means the estimate Q will be very off from the true uniform P. After 100 dice rolls, the KL Divergence between Q and P could be quite large:\n",
    "\n",
    "[Pics with example numbers for Q.]\n",
    "[Make up p_i for i in range(1, 7)]\n",
    "\n",
    "Had the researchers given us pen and paper and told us we could write the numbers down, then we would likely make better guesses in both the coin and dice examples. Writing down the numbers and averaging them later is similar to using a new model for Q that can get closer to the real P. This shows how KL Divergence can be used for model selection, since models with divergence get closer to the real distribution and are thus better choices. \n",
    "\n",
    "\n",
    "Notes: Could say more on KL and Variational, need to find right split between that.\n",
    "Flesh out entropy argument in definition more. \n",
    "\n",
    "\n",
    "Resources:\n",
    "1. https://medium.com/@samsachedina/demystified-kullback-leibler-divergence-3971f956ef34\n",
    "2. https://www.quora.com/What-is-a-good-laymans-explanation-for-the-Kullback-Leibler-divergence\n",
    "3. http://web.engr.illinois.edu/~hanj/cs412/bk3/KL-divergence.pdf\n",
    "4. https://www.johndcook.com/blog/2017/11/08/why-is-kullback-leibler-divergence-not-a-distance/\n",
    "5. https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
