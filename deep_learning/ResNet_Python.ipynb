{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Pre-Activation Wide-ResNet Network for digit classification.\n",
    "\n",
    "References:\n",
    "    Basic ResNet: https://arxiv.org/pdf/1512.03385.pdf\n",
    "    Pre-Act. ResNet: https://arxiv.org/pdf/1603.05027.pdf\n",
    "    Wide-ResNet: https://arxiv.org/pdf/1605.07146v1.pdf\n",
    "    \n",
    "In this post we build a Residual Network (ResNet) in TensorFlow with python to classify the handwritten digits of the popular MNIST database. We create a pre-activation ResNet that performs better than its original counterpart [2]. The network is created as a python class following some of the best-practice flavors that are starting to emerge.\n",
    "\n",
    "First we define some utility functions that translate well across networks and tasks, helping future projects get off the ground that much quicker. Then we show the actual code with comments explaining the overall structure and most crucial parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create a decorator that wraps around specific portions of our overall TensorFlow graph. The decorator makes sure that the code which creates the nodes and operations for the predictions, loss function, and optimizers is only ran once. This concept is known as lazy initialization, where some object or data structure is created only when it is needed. The code for the decorator is small yet handy. We can extend it to work with TensorFlow's variable scopes and arbitrary argument lists. Combined with helper functions for common layers and operations, the decorator helps us cut down on boilerplate code and make the network much more readable and understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def lazy_init(function):\n",
    "    '''\n",
    "    Lazy initialization for inference, loss, and optimzier in a graph.\n",
    "    '''\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a named tuple for our hyperparameters. This is cleaner and less error-prone than passing in a dictionary. Combined with `tf.FLAGS`, it is the ideal way of handling the multitude of parameters in a Deep Learning project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# tuple for model hyperparameters\n",
    "HyperParams = namedtuple('HyperParams',\n",
    "                     'batch_size, num_cls, num_chans_in, height, width, filter_dims,'\n",
    "                     'lr, kernel_dims, strides, n_hidden, relu_alpha')\n",
    "# sample hyperparameters\n",
    "h = HyperParams(128, 10, 1, 20, 512, [16, 16, 16, 32, 64], 1e-5, [3, 3],\n",
    "                [1, 1, 1, 2, 2], 128, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define some constants for Batch Normalization that have proven to be reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters for batch_norm\n",
    "_BATCH_NORM_DECAY = 0.997\n",
    "_BATCH_NORM_EPSILON = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to build on pre-activation ResNet in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ResNet(object):\n",
    "    def __init__(self, inputs, labels, is_training, hps):\n",
    "        '''\n",
    "        Builds a 5-layer pre-activation ResNet.\n",
    "        Using Xavier initializer for conv and dense weights.\n",
    "        Leaky-relu as activation function.\n",
    "        '''\n",
    "        # dimensions and sizes for input and label shapes\n",
    "        self.batch_size = hps.batch_size\n",
    "        self.num_cls = hps.num_cls\n",
    "\n",
    "        # store placeholders for inputs, labels, and is_training flag\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.is_training = is_training\n",
    "\n",
    "        # check data format for convolutions and batch norm\n",
    "        # makes a difference when code is run with a GPU.\n",
    "        self.data_format = 'channels_first' if tf.test.is_built_with_cuda() \\\n",
    "                                            else 'channels_last'\n",
    "\n",
    "        # network hyperparameters\n",
    "        self.n_filts = hps.filter_dims\n",
    "        self.kernel_dim = hps.kernel_dims\n",
    "        self.strides = hps.strides\n",
    "        self.n_hidden = hps.n_hidden\n",
    "        self.relu_alpha = hps.relu_alpha\n",
    "        # learning rate\n",
    "        self.lr = hps.lr\n",
    "        # initializer for weight layers\n",
    "        self.weight_init = tf.variance_scaling_initializer()\n",
    "\n",
    "        # subgraphs that will be created by lazy_init\n",
    "        self.logits\n",
    "        self.loss\n",
    "        self.train_op\n",
    "\n",
    "    @lazy_init\n",
    "    def logits(self):\n",
    "        x = self._build_first_layer()\n",
    "        x = self._build_resnet(x)\n",
    "        return x\n",
    "\n",
    "    def _build_first_layer(self):\n",
    "        '''\n",
    "        Return the first convolutional layer common to all ResNet models.\n",
    "\n",
    "        Note: Slightly different from layer + pooling in original resnet, but\n",
    "            this performs better.\n",
    "        '''\n",
    "        with tf.variable_scope('first_layer'):\n",
    "            if self.data_format == 'channels_first':\n",
    "                self.inputs = tf.transpose(self.inputs, [0, 3, 2, 1])\n",
    "            x = self._conv_layer(self.inputs, 16, 3, 1)\n",
    "            return x\n",
    "\n",
    "    def _build_resnet(self, x):\n",
    "        '''Builds the residual blocks.'''\n",
    "\n",
    "        with tf.variable_scope('first_enc_block'):\n",
    "            x = self._resnet_block(x, 16, self.n_filts[0], self.strides[0])\n",
    "\n",
    "        with tf.variable_scope('second_enc_block'):\n",
    "            x = self._resnet_block(x, self.n_filts[0], self.n_filts[1],\n",
    "                self.strides[1])\n",
    "\n",
    "        with tf.variable_scope('third_enc_block'):\n",
    "            x = self._resnet_block(x, self.n_filts[1], self.n_filts[2],\n",
    "                self.strides[2])\n",
    "\n",
    "        with tf.variable_scope('fourth_enc_block'):\n",
    "            x = self._resnet_block(x, self.n_filts[2], self.n_filts[3],\n",
    "                self.strides[3])\n",
    "\n",
    "        with tf.variable_scope('GAP_block'):\n",
    "            gap_dims = [2, 3] if self.data_format == 'channels_first' \\\n",
    "                                else [1, 2]\n",
    "            x = tf.reduce_mean(x, gap_dims)\n",
    "\n",
    "        with tf.variable_scope('dense'):\n",
    "            x = tf.reshape(x, [self.batch_size, -1])\n",
    "            x = tf.layers.dense(x, self.n_hidden, activation=self._leaky_relu,\n",
    "                    kernel_initializer=self.weight_init)\n",
    "\n",
    "        with tf.variable_scope('logits'):\n",
    "            x = tf.layers.dense(x, self.num_cls,\n",
    "                kernel_initializer=self.weight_init)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @lazy_init\n",
    "    def loss(self):\n",
    "        with tf.variable_scope('loss'):\n",
    "            x_ent = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=self.labels,\n",
    "                logits=self.logits)\n",
    "            loss = tf.reduce_mean(x_ent)\n",
    "            tf.summary.scalar('loss', loss)\n",
    "        return loss\n",
    "\n",
    "    @lazy_init\n",
    "    def train_op(self):\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            # learning rate schedule\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                learning_rate=self.lr, global_step=global_step,\n",
    "                decay_steps=10000, decay_rate=0.97, staircase=True)\n",
    "            tf.summary.scalar('learn_rate', learning_rate)\n",
    "            # Nesterov momentum optimizer\n",
    "            optim = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                momentum=.9, use_nesterov=True)\n",
    "            # add optimizers for batch_norm\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(update_ops):\n",
    "                train_op = optim.minimize(self.loss, global_step=global_step)\n",
    "        self.summaries = tf.summary.merge_all()\n",
    "        return train_op\n",
    "\n",
    "    def _resnet_block(self, inputs, in_filts, out_filts, strides):\n",
    "        '''\n",
    "        ResNet block with two convolutions.\n",
    "        Projects original inputs through 1x1 conv if strides downsample.\n",
    "        '''\n",
    "        # store original x for skip-connection\n",
    "        orig_in = inputs\n",
    "\n",
    "        # project input if we are downsampling\n",
    "        if in_filts != out_filts:\n",
    "            # 1x1 conv, downsample by 2, output number of filters\n",
    "            orig_in = self._conv_layer(orig_in, out_filts, 1, 2)\n",
    "            # strides to downsample in encoder conv layers\n",
    "            first_stride, second_stride = 2, 1\n",
    "        else:\n",
    "            first_stride, second_stride = strides, strides\n",
    "\n",
    "        # first pre-activation block\n",
    "        inputs = self._batch_norm_relu(inputs)\n",
    "        inputs = self._conv_layer(inputs, num_filts, self.kernel_dim,\n",
    "            first_stride)\n",
    "\n",
    "        # second pre-activation block\n",
    "        inputs = self._batch_norm_relu(inputs)\n",
    "        inputs = self._conv_layer(inputs, num_filts, self.kernel_dim,\n",
    "            second_stride)\n",
    "\n",
    "        # add back original (possibly projected) inputs\n",
    "        inputs += orig_in\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def _batch_norm_relu(self, inputs):\n",
    "        '''\n",
    "        Passes inputs through batch normalization and leaky-relu.\n",
    "        '''\n",
    "        inputs = tf.layers.batch_normalization(inputs,\n",
    "            axis=1 if self.data_format == 'channels_first' else 3,\n",
    "            momentum=_BN_DECAY, epsilon=_BN_EPSILON, center=True, scale=True,\n",
    "            training=self.is_training, fused=True)\n",
    "        inputs = self._leaky_relu(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def _conv_layer(self, inputs, n_filts, kernel, strides, padding='same'):\n",
    "        return tf.layers.conv2d(inputs, n_filts, kernel, strides, padding=padding,\n",
    "            data_format=self.data_format, kernel_initializer=self.weight_init)\n",
    "\n",
    "    def _leaky_relu(self, inputs):\n",
    "        return tf.maximum(self.relu_alpha * inputs, inputs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
